---
title: "Agent Definition and Usage"
sidebar_label: "Agent Usage"
sidebar_position: 140
description: "Configuring and using the Agent class"
---

에이전트는 모델, 프롬프트, 툴 및 기타 설정을 캡슐화합니다. 전역으로 정의하거나 런타임에 정의할 수 있습니다.

에이전트는 스레드를 사용하여 사용자, 다른 에이전트/LLM 또는 다른 곳에서 온 메시지 시리즈를 포함합니다. 스레드는 여러 에이전트가 응답하거나 단일 에이전트가 사용할 수 있습니다.

에이전트 워크플로우는 컨텍스트 프롬프팅(스레드, 메시지, 툴 응답, RAG 등)과 LLM 툴 호출, 구조화된 LLM 출력 또는 커스텀 코드를 통한 다양한 기술을 통한 동적 라우팅을 결합하여 구축됩니다.

## 기본 에이전트 정의

```ts
import { components } from "./_generated/api";
import { Agent } from "@convex-dev/agent";
import { openai } from "@ai-sdk/openai";

const agent = new Agent(components.agent, {
  name: "Basic Agent",
  languageModel: openai.chat("gpt-4o-mini"),
});
```

더 많은 설정 옵션은 [아래](#customizing-the-agent)를 참조하세요.

이름을 제외한 모든 것은 LLM 호출 시 호출 위치에서 재정의할 수 있으며, 에이전트에서 사용 가능한 많은 기능은 이러한 작업 구성 방식이 필요하지 않은 경우 에이전트 없이 사용할 수 있습니다.

## 동적 에이전트 정의

특정 컨텍스트에 대한 에이전트를 만들려는 경우 런타임에 에이전트를 정의할 수 있습니다. 이를 통해 LLM이 각 툴 호출에 전체 컨텍스트를 항상 전달하지 않고도 툴을 호출할 수 있습니다. 또한 에이전트에 대한 모델이나 기타 옵션을 동적으로 선택할 수 있습니다.

```ts
import { Agent } from "@convex-dev/agent";
import { type LanguageModel } from "ai";
import type { ActionCtx } from "./_generated/server";
import type { Id } from "./_generated/dataModel";
import { components } from "./_generated/api";

function createAuthorAgent(
  ctx: ActionCtx,
  bookId: Id<"books">,
  model: LanguageModel,
) {
  return new Agent(components.agent, {
    name: "Author",
    languageModel: model,
    tools: {
      // https://docs.convex.dev/agents/tools 참조
      getChapter: getChapterTool(ctx, bookId),
      researchCharacter: researchCharacterTool(ctx, bookId),
      writeChapter: writeChapterTool(ctx, bookId),
    },
    maxSteps: 10, // stopWhen: stepCountIs(10)의 대안
  });
}
```

## 에이전트로 텍스트 생성하기

메시지를 생성하려면 프롬프트(문자열 또는 메시지 목록)를 제공하여 `agent.streamText` 또는 `agent.generateObject`와 같은 호출을 사용하여 LLM을 통해 하나 이상의 메시지를 생성하는 컨텍스트로 사용됩니다.

`generateText` 및 기타 함수의 인수는 AI SDK와 동일하지만, 모델을 제공할 필요가 없습니다. 기본적으로 에이전트의 언어 모델을 사용합니다. 또한 아래에서 볼 수 있는 `promptMessageId`와 같이 에이전트 컴포넌트에 특정한 추가 인수도 있습니다.

[**여기에서 AI SDK 인수 전체 목록을 확인하세요**](https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-text)

메시지 히스토리는 기본적으로 주어진 [스레드](./threads.mdx)의 컨텍스트로 제공됩니다. 제공되는 컨텍스트를 구성하는 방법에 대한 자세한 내용은 [LLM 컨텍스트](./context.mdx)를 참조하세요.

참고: 아래에서 참조되는 `authorizeThreadAccess`는 사용자가 스레드에 액세스할 수 있도록 인증하고 권한을 부여하기 위해 작성할 함수입니다. [threads.ts](https://github.com/get-convex/agent/blob/main/example/convex/threads.ts)에서 예제 구현을 볼 수 있습니다.

실시간 코드 예제는 [chat/basic.ts](https://github.com/get-convex/agent/blob/main/example/convex/chat/basic.ts) 또는 [chat/streaming.ts](https://github.com/get-convex/agent/blob/main/example/convex/chat/streaming.ts)를 참조하세요.

### 텍스트 스트리밍

텍스트 스트리밍은 아래 접근 방식과 동일한 패턴을 따르지만 수행하는 스트리밍 유형에 따라 몇 가지 차이점이 있습니다. 자세한 내용은 [스트리밍](./streaming.mdx)을 참조하세요.

### 기본 접근 방식 (동기식)

```ts
export const generateReplyToPrompt = action({
  args: { prompt: v.string(), threadId: v.string() },
  handler: async (ctx, { prompt, threadId }) => {
    // await authorizeThreadAccess(ctx, threadId);
    const result = await agent.generateText(ctx, { threadId }, { prompt });
    return result.text;
  },
});
```

참고: 액션에서 데이터를 반환하는 것에 의존하지 않는 것이 모범 사례입니다. 대신 `useThreadMessages` 훅을 통해 스레드 메시지를 쿼리하고 새 메시지를 자동으로 받으세요. 아래를 참조하세요.

### 프롬프트를 저장한 다음 비동기적으로 응답 생성하기

위의 접근 방식은 간단하지만 비동기적으로 응답을 생성하면 몇 가지 이점이 있습니다:

- 트랜잭션이므로 뮤테이션에서 낙관적 UI 업데이트를 설정할 수 있으므로 메시지가 저장되고 메시지 쿼리에 표시될 때까지 클라이언트에서 낙관적으로 표시됩니다.
- 데이터베이스에 대한 다른 쓰기와 동일한 뮤테이션(트랜잭션)에 메시지를 저장할 수 있습니다. 이 메시지는 히스토리에서 프롬프트 메시지를 복제하지 않고 재시도와 함께 액션에서 사용 및 재사용될 수 있습니다. `promptMessageId`가 여러 생성에 사용되는 경우 이전 응답이 자동으로 컨텍스트로 포함되므로 LLM이 중단한 곳에서 계속할 수 있습니다. 자세한 내용은 [워크플로우](./workflows.mdx)를 참조하세요.
- 뮤테이션의 멱등성 보장 덕분에 클라이언트는 정확히 한 번 실행될 때까지 며칠 동안 안전하게 뮤테이션을 재시도할 수 있습니다. 액션은 일시적으로 실패할 수 있습니다.

메시지를 나열하는 모든 클라이언트는 비동기적으로 생성되는 새 메시지를 자동으로 받습니다.

비동기적으로 응답을 생성하려면 먼저 메시지를 저장한 다음 `messageId`를 `promptMessageId`로 전달하여 텍스트를 생성/스트리밍해야 합니다.

```ts
import { components, internal } from "./_generated/api";
import { saveMessage } from "@convex-dev/agent";
import { internalAction, mutation } from "./_generated/server";
import { v } from "convex/values";

// 1단계: 사용자 메시지를 저장하고 비동기 응답을 시작합니다.
export const sendMessage = mutation({
  args: { threadId: v.id("threads"), prompt: v.string() },
  handler: async (ctx, { threadId, prompt }) => {
    const { messageId } = await saveMessage(ctx, components.agent, {
      threadId,
      prompt,
    });
    await ctx.scheduler.runAfter(0, internal.example.generateResponseAsync, {
      threadId,
      promptMessageId: messageId,
    });
  },
});

// 2단계: 사용자 메시지에 대한 응답을 생성합니다.
export const generateResponseAsync = internalAction({
  args: { threadId: v.string(), promptMessageId: v.string() },
  handler: async (ctx, { threadId, promptMessageId }) => {
    await agent.generateText(ctx, { threadId }, { promptMessageId });
  },
});
```

액션이 아무것도 반환할 필요가 없습니다. 모든 메시지는 기본적으로 저장되므로 스레드 메시지를 구독하는 모든 클라이언트는 비동기적으로 생성되는 새 메시지를 받습니다.

2단계 코드는 충분히 일반적이어서 타이핑을 절약할 수 있는 유틸리티가 있습니다. 스트리밍 등을 제어하는 일부 매개변수를 받습니다. 자세한 내용은 [코드](https://github.com/get-convex/agent/blob/main/src/client/index.ts#L1475-L1557)를 참조하세요.

```ts
// 위의 2단계와 동일합니다.
export const generateResponseAsync = agent.asTextAction();
```

### 객체 생성하기

AI SDK와 유사하게 객체를 생성하거나 스트리밍할 수 있습니다. 동일한 인수가 적용되지만 모델을 제공할 필요가 없습니다. 에이전트의 기본 언어 모델을 사용합니다.

```ts
import { z } from "zod/v3";

const result = await thread.generateObject({
  prompt: "지금까지의 대화를 기반으로 계획을 생성하세요",
  schema: z.object({...}),
});
```

안타깝게도 객체 생성은 툴 사용을 지원하지 않습니다. 그러나 한 가지 방법은 객체를 반환하는 툴 호출의 인수로 객체를 구조화하는 것입니다. 커스텀 `stopWhen`을 사용하여 툴 호출이 결과를 생성할 때 생성을 중지하고 `toolChoice: "required"`를 사용하여 LLM이 텍스트 응답을 반환하는 것을 방지할 수 있습니다.

## 에이전트 커스터마이징

에이전트는 기본적으로 `chat` 모델만 구성하면 됩니다. 그러나 벡터 검색의 경우 `textEmbeddingModel` 모델이 필요합니다. `name`은 각 메시지를 특정 에이전트에 귀속시키는 데 도움이 됩니다. 다른 옵션은 각 LLM 호출 위치에서 재정의할 수 있는 기본값입니다.

```ts
import { tool, stepCountIs } from "ai";
import { openai } from "@ai-sdk/openai";
import { z } from "zod/v3";
import { Agent, createTool, type Config } from "@convex-dev/agent";
import { components } from "./_generated/api";

const sharedDefaults = {
  // 에이전트에 사용할 언어 모델
  languageModel: openai.chat("gpt-4o-mini"),
  // 메시지 히스토리의 벡터 검색을 지원하는 임베딩 모델 (RAG)
  textEmbeddingModel: openai.embedding("text-embedding-3-small"),
  // 컨텍스트 메시지 가져오기에 사용됩니다. https://docs.convex.dev/agents/context 참조
  contextOptions,
  // 메시지 저장에 사용됩니다. https://docs.convex.dev/agents/messages 참조
  storageOptions,
  // 토큰 사용량 추적에 사용됩니다. https://docs.convex.dev/agents/usage-tracking 참조
  usageHandler: async (ctx, args) => {
    const { usage, model, provider, agentName, threadId, userId } = args;
    // ... 로그, 데이터베이스에 사용량 저장 등
  },
  // 컨텍스트 메시지 필터링, 수정 또는 보강에 사용됩니다. https://docs.convex.dev/agents/context 참조
  contextHandler: async (ctx, args) => {
    return [...customMessages, args.allMessages];
  },
  // 모든 요청과 응답을 로그하거나 기록하려는 경우 유용합니다.
  rawResponseHandler: async (ctx, args) => {
    const { request, response, agentName, threadId, userId } = args;
    // ... 로그, 데이터베이스에 요청/응답 저장 등
  },
  // 툴 호출이 실패할 때 재시도 횟수를 제한하는 데 사용됩니다. 기본값: 3
  callSettings: { maxRetries: 3, temperature: 1.0 },
} satisfies Config;


const supportAgent = new Agent(components.agent, {
  // 재정의되지 않는 경우 기본 시스템 프롬프트
  instructions: "당신은 유용한 어시스턴트입니다.",
  tools: {
    // Convex 툴. https://docs.convex.dev/agents/tools 참조
    myConvexTool: createTool({
      description: "My Convex tool",
      args: z.object({...}),
      // 참고: 타입 순환을 피하기 위해 핸들러의 반환 타입을 주석으로 표시하세요.
      handler: async (ctx, args): Promise<string> => {
        return "Hello, world!";
      },
    }),
    // 표준 AI SDK 툴
    myTool: tool({ description, parameters, execute: () => {}}),
  },
  // 툴 호출이 포함될 때 단계 수를 제한하는 데 사용됩니다.
  // 참고: 단일 호출로 툴 호출이 자동으로 발생하도록 하려면
  // 이 값을 1(기본값)보다 크게 설정해야 합니다.
  stopWhen: stepCountIs(5),
  ...sharedDefaults,
});
```
